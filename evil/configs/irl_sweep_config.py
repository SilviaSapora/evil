import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"

sweep_configuration = {
    "method": "grid",
    "metric": {"goal": "maximize", "name": "return_and_retrain"},
    "parameters": {
        "irl_plus": {"value": True},
        "log_gen_every": {"value": 2},
        "reward_net_hsize": {"value": [128, 128]},
        "reward_net_sigmoid": {"value": True},
        "wandb_log": {"value": True},
        "plot": {"value": False},
        "train_rng": {"value": "DIFFERENT_IN_PAIRS"},
        "save_to_file": {"value": True},
        "env": {"value": "ant"},
        "reward_normalize": {"value": True},
        "reward_all_normalize": {"value": False},
        "obs_normalize": {"value": True},
        "num_eval_steps": {"value": 1000},
        "num_expert_eval_envs": {"value": 50},
        "loss": {"value": "IRL"},
        "seed": {"value": 1},
        "seeds": {"value": 5},
        "num_discr": {"value": 5},
        "dual": {"value": False},
        "run_test": {"value": False},
        "num_eval_envs": {"value": 50},
        "inner_lr_linear": {"value": False},
        "inner_lr": {"value": 3e-4},
        "inner_steps": {"value": 10},
        # SHAPING
        "real_reward": {"value": "GROUND_TRUTH_REWARD"},
        "reward_type": {"value": "REWARD_STATE"},
        # IRL
        "discr_loss": {"value": "bce"},
        "alpha": {"value": 0},
        "irl_generations": {"value": 2441},
        "discr_l2_loss": {"value": 0.0},
        "discr_batch_size": {"value": 4096},
        "irl_lrate_init": {"value": 4e-3},
        "discr_schedule_type": {"value": "linear"},
        "discr_trans_decay": {"value": 400},
        "discr_final_lr_diff": {"value": 4},
        "discr_updates_every": {"value": 1},
        "discr_updates": {"value": 20},
        "num_updates_inner_loop": {"value": 1},
        "num_updates_two_step": {"value": 1000},
        "retrain_from_scratch": {"value": True},
        "grad_penalty_coeff": {"value": 10},
        "backend": {"value": "positional"},
        "backend_test": {"value": "positional"},
        "buffer_size_perc": {"value": 1},
        "reward_net_ensemble_type": {"value": "avg"},
        "reward_net_ensemble_params_type": {"value": "same_agent"},
        "expert_num_seeds": {"value": 1},
        "random_reinit": {"value": False},
        "random_reinit_prob": {"value": 0.03},
        "random_reinit_prob_final": {"value": 2},
        "random_reinit_decay": {"value": "linear"},
    },
}
